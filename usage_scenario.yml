name: Energy Measurement Ollama
author: envite Consulting GmbH <ruessmann|wittrowski|kepes@envite.de>
description: usage_scenario for Energy Measurement Ollama

compose-file: !include docker-compose.yml

services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"  # Standard-Port f√ºr Ollama API
    #volumes:
    #  - ollama_data:/root/.ollama  # Speichert Modelle & Daten persistent
    restart: unless-stopped
    
  curl:
    container_name: curl
    image: curlimages/curl:8.1.0
    command: sh

flow:
    - name: run Experiment
      container: curl
      commands:
      - type: console
        command: |
          curl -s ollama:11434/api/pull -d '{ \"model\": \"llama3.2:1b\" }'
        note: Get Model
        log-stdout: true
        log-stderr: true
      - type: console
        command: |
          curl -s ollama:11434/api/generate -d '{  "model": "llama3.2:1b",  "prompt":"Why is the sky blue?"}'
        note: Send Prompt
        log-stdout: true
        log-stderr: true
